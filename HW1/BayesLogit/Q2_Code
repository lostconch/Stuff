library(MASS)

# read the data (The data file "blr_data_1004.csv" was downloaed from the Github repo)
sim_num <- 1004
mydata <- read.csv(paste("Desktop/blr_data_",sim_num,".csv",sep=""))

# extract the values of m, y, and X from the data
m<-mydata$n
y<-mydata$y
X<-matrix(c(mydata$X1,mydata$X2),ncol = 2)

# this function computes the logarithm of the target probability density
post <- function(beta,mu_0,sigma_0,m,y,X){
	q <- -1/2*t(beta-mu_0)%*%sigma_0%*%(beta-mu_0)
	u <- X%*%beta
    for (i in 1:length(y)){
    	q <- q + y[i]*u[i] - m[i]*log(1+exp(u[i]))
    }
    return(q)
}

# this main function implements an M-H algorithm
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv,niter,burnin,print.every,retune,verbose){
	
	# intitialization of the proposal standard deviation
	c <-2.38/sqrt(length(beta.0))
	
	# B will store all the parameter values generated after the burnin period in a niter-by-2 matrix
	B <- matrix(rep(0,2*niter),ncol=2)
	
	# intitialization of the parameter beta
	beta <- rep(0,2)
	
	# s is the number of acceptance within a tuning loop
	s <- 0
	
	# total number of M-H iterations: burnin + niter
	for (i in 1:(burnin+niter)){
		
		# sample from the proposal distribution, which is a 2-dim Gaussian
		beta_1=mvrnorm(n=1, beta, c^2*diag(1,2))
		
		# condition of acceptance
		if (log(runif(1)) < post(beta_1,beta.0,Sigma.0.inv,m,y,X) - post(beta,beta.0,Sigma.0.inv,m,y,X)){
			beta <- beta_1
			
			# increase s by 1 if accepted
			if (i<=burnin){
				s <- s+1
			}
		}
		# begin to record the simulation results after the burnin period is over
		if (i>burnin){
				B[i-burnin,] <- beta
			}
			
			# print the parameter value every "print.every" steps
		if (i%%print.every==0){
			print(beta)
		}
		
		# tune the proposal standard deviation every "retune" steps in the burnin period
		if (i%%retune==0 && i<=burnin){		
			
			# s/retune gives the acceptance rate 
			if (s/retune<0.3 || s/retune>0.6){
				
				# Increase/decrease the c value if the acceptance rate is too high/low				
				c <- c*qnorm(0.45/2)/qnorm(s/(retune+1)/2)
			}	
			
			# reset s to 0 at the end of each tuning loop
		s <- 0
		}
	}
	
	# If "verbose" is TRUE then print warning messages
	if (verbose==TRUE){
		print(warnings())
	}
	return(B)
}

# call the main function "bayes.logreg"
z <- "bayes.logreg"(m,y,X,beta.0=c(0,0),Sigma.0.inv=diag(1,2),niter=10000,burnin=1000,print.every=1000,retune=100,verbose=TRUE)

# compute the 1, 2, ..., 99% quantiles of the marginal posterior distributions of beta
a1 <- quantile(z[,1], 1/100*c(1:99), names=FALSE)
a2 <- quantile(z[,2], 1/100*c(1:99), names=FALSE)

# output the quantiles to a .csv file with no header
write.table(matrix(c(a1,a2),ncol=2), file = paste("Desktop/blr_res_",sim_num,".csv",sep=""), sep=",", col.names=F, row.names=F)
